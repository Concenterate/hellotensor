{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Canton\n",
    "\n",
    "The Canton library is a lightweight wrapper around TensorFlow, focused on **parameter sharing**. The author of this library is a deep learning guy who has experienced both Torch, TensorFlow and Keras.\n",
    "\n",
    "Canton is named after the city of Guangzhou. The French came a long time ago; they used to call this city \"Canton\", which sounds like \"Guangdong\" when pronounced in French, which is actually the name of the province, not the city. Since then, all westerners start to use the word Canton. The Yue language, a dialect of Chinese commonly used in Guangzhou and the United States, is known as \"Cantonese\" in English for this reason.\n",
    "\n",
    "## The Canton Philosophy\n",
    "\n",
    "- The network units, and the weights associated with them, should be tied together as one and not seperated.\n",
    "- Obtaining the weight tensors of any given action(or a set of actions bound together) should be as easy as calling `some_action.get_weights()`, not `tf.very_long_method_name(some_collection).some_other_method(some_name_prefixes)`.\n",
    "- You should by default be able to use a unit everywhere, while maintaining only one set of weights for that unit.\n",
    "\n",
    "## Story Behind\n",
    "\n",
    "TensorFlow is cool in general, but some of its designs are disasterous. The official way to share variables(weights) between copies of networks is to use `tf.variable_scope(scopename, reuse=True)` and `tf.get_variable(name)`. It then became the programmer's responsibility to specify(and keep track of) the scope names, variable names and flags. As a programmer, I soon realized that *There are only two hard things in Computer Science: cache invalidation and naming things.*\n",
    "\n",
    "TensorFlow is from Google, where CS PhDs write all the code, so that mustn't be their problem. In order to deal with my own incompetence, I wrote this library.\n",
    "\n",
    "> Keras also wrapped the quirks and weirdness of TensorFlow and allows for rapid prototyping, but if you want to introduce your own calculation and/or manipulation operations into the model, you must first inherit Keras' Layer class, then (in some cases) specify a shape inference function, which is boring and inefficient. Besides that, Keras does not support anything other than the kaggle-styled, input-to-output-chained, one-loss-updates-everything architecture. I tried various method to wrap aroud Keras(in order to add my own functionality), but the internal complexity of Keras continuously freaked me off(I read almost every page of its documentation and half its code).\n",
    "\n",
    "> Other learning frameworks also made various attempts on solving the same problem, using fancy descriptions like \"imperative vs declarative\". Well, maybe they do need a lot of PhDs to solve the *second hardest thing in Computer Science...*\n",
    "\n",
    "## Install\n",
    "\n",
    "pip install canton\n",
    "\n",
    "## Usage\n",
    "\n",
    "Input the essentials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import canton as ct\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_variable = tf.Variable(np.random.normal(loc=0,scale=1,size=[1,256,256,3]\n",
    "    ).astype('float32'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then feed it through three 2-D convolutional layers, where:\n",
    "- conv_0 has its own weights\n",
    "- conv_1 and conv_2 share weights\n",
    "\n",
    "In order to do this we first create 2 convolutional layers, each with its own set of weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tensorflow.python.ops.variables.Variable object at 0x00000000092C3908>, <tensorflow.python.ops.variables.Variable object at 0x00000000092E7550>]\n",
      "[<tensorflow.python.ops.variables.Variable object at 0x00000000092FFB38>, <tensorflow.python.ops.variables.Variable object at 0x00000000092FAB70>]\n"
     ]
    }
   ],
   "source": [
    "conv = ct.Conv2D(3,16,3)\n",
    "shared_conv = ct.Conv2D(16,16,3)\n",
    "print(conv.weights)\n",
    "print(shared_conv.weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then simply apply the second layer twice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"add_2:0\", shape=(1, 256, 256, 16), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "i = conv(input_variable)\n",
    "i = shared_conv(i)\n",
    "out = shared_conv(i)\n",
    "print(out)\n",
    "\n",
    "# define loss\n",
    "loss = tf.reduce_mean(out**2.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's assume you only want to train the shared layer's weights (keep the first `conv` layer's weight frozen). Instead of using `tf.get_collection(some_keys_you_have_to_remember)`, or `get_layer('some_name').trainable = False`, you simply pick the weights you want to train and throw them into `optimizer.minimize()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define optimizer\n",
    "opt = tf.train.AdamOptimizer(1e-3)\n",
    "# define train op\n",
    "train_step = opt.minimize(loss,var_list=shared_conv.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems stupid (doing more work than Keras) at first glance, but super handy if you happen to be training GANs or anything NOT for Kaggle competitions.\n",
    "\n",
    "Now you can train it the TensorFlow way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.266927\n",
      "loss: 0.251622\n",
      "loss: 0.237126\n",
      "loss: 0.223424\n",
      "loss: 0.210489\n",
      "loss: 0.198296\n",
      "loss: 0.186811\n",
      "loss: 0.176009\n",
      "loss: 0.165861\n",
      "loss: 0.156332\n"
     ]
    }
   ],
   "source": [
    "sess = ct.get_session() # just the TF Session\n",
    "sess.run(tf.global_variables_initializer()) # initialize all weights\n",
    "for i in range(10):\n",
    "    res = sess.run([train_step,loss],feed_dict={}) # you should feed inputs if you have\n",
    "    print('loss:',res[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok the loss is decreasing, which means the weights are getting trained. Now let's assume you like this \"2Conv1Weight\" idea very much, and wanna apply this layer two more times to your model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out = shared_conv(out)\n",
    "out = shared_conv(out)\n",
    "\n",
    "# redefine loss\n",
    "loss = tf.reduce_mean(out**2.)\n",
    "# redefine train op (Note: do not redefine the optimizer, which will produce error due to variable scope clasing)\n",
    "train_step = opt.minimize(loss,var_list=shared_conv.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't have to reinitialize all the variables, since the previous session is still open:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.131151\n",
      "loss: 0.118935\n",
      "loss: 0.107218\n",
      "loss: 0.0963017\n",
      "loss: 0.0863248\n",
      "loss: 0.0773284\n",
      "loss: 0.0692928\n",
      "loss: 0.0621648\n",
      "loss: 0.0558718\n",
      "loss: 0.0503311\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    res = sess.run([train_step,loss],feed_dict={})\n",
    "    print('loss:',res[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the loss value, the weights are not lost between two runs. Now let's assume you wanna save the weights to file(in numpy format) for future uses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights obtained.\n",
      "successfully saved to shared_conv.npy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shared_conv.save_weights('shared_conv.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you train the model for some more steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.0454626\n",
      "loss: 0.041187\n",
      "loss: 0.0374314\n",
      "loss: 0.0341301\n",
      "loss: 0.031224\n",
      "loss: 0.0286602\n",
      "loss: 0.0263941\n",
      "loss: 0.0243854\n",
      "loss: 0.0226004\n",
      "loss: 0.0210091\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    res = sess.run([train_step,loss],feed_dict={})\n",
    "    print('loss:',res[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the loss is too low, showing signs of overfitting. Assume you want to revert your weights to the last checkpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully loaded from shared_conv.npy\n",
      "2 weights assigned.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shared_conv.load_weights('shared_conv.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train the model again. As you can see the loss values increased back to our previous checkpoint. (However the training dynamic governed by the Adam optimizer didn't change, so the results are not going to be exactly identical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.0454626\n",
      "loss: 0.0422226\n",
      "loss: 0.0391709\n",
      "loss: 0.0363211\n",
      "loss: 0.0336778\n",
      "loss: 0.0312398\n",
      "loss: 0.0290012\n",
      "loss: 0.0269524\n",
      "loss: 0.025082\n",
      "loss: 0.0233781\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    res = sess.run([train_step,loss],feed_dict={})\n",
    "    print('loss:',res[1])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
